{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "l7DcfOpivHea"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/swarm.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'PASTE YOUR OPENAI API KEY'"
      ],
      "metadata": {
        "id": "VnJcrzLEvdid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Agents and Handoffs Functions"
      ],
      "metadata": {
        "id": "nED06Im9buot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from swarm import Swarm, Agent\n",
        "\n",
        "def handoff_to_weather_agent():\n",
        "    \"\"\"Transfer to the weather agent for weather queries.\"\"\"\n",
        "    print(\"Handing off to Weather Agent\")\n",
        "    return weather_agent\n",
        "\n",
        "def handoff_to_math_agent():\n",
        "    \"\"\"Transfer to the math agent for mathematical queries.\"\"\"\n",
        "    print(\"Handing off to Math Agent\")\n",
        "    return math_agent\n",
        "\n",
        "math_agent = Agent(\n",
        "    name=\"Math Agent\",\n",
        "    instructions=\"You handle only mathematical queries.\",\n",
        "    functions = [handoff_to_weather_agent]\n",
        ")\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"Weather Agent\",\n",
        "    instructions=\"You handle only weather-related queries.\",\n",
        "    functions = [handoff_to_math_agent]\n",
        ")"
      ],
      "metadata": {
        "id": "OQg2KeJswOde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Swarm()\n",
        "messages = [{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n",
        "handoff_response = client.run(agent=weather_agent, messages=messages)\n",
        "print(handoff_response)\n",
        "print(\"=============\")\n",
        "print(\"=============\")\n",
        "print(handoff_response.messages[-1][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNNoUR8KxH_G",
        "outputId": "7c005338-c8d8-4e35-ff9b-dbc095c7d1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handing off to Math Agent\n",
            "messages=[{'content': None, 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'call_lgi8IPE2s4yifgXLXAMiuxNP', 'function': {'arguments': '{}', 'name': 'handoff_to_math_agent'}, 'type': 'function'}], 'sender': 'Weather Agent'}, {'role': 'tool', 'tool_call_id': 'call_lgi8IPE2s4yifgXLXAMiuxNP', 'tool_name': 'handoff_to_math_agent', 'content': '{\"assistant\": \"Math Agent\"}'}, {'content': 'The answer to 2 + 2 is 4.', 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None, 'sender': 'Math Agent'}] agent=Agent(name='Math Agent', model='gpt-4o', instructions='You handle only mathematical queries.', functions=[<function handoff_to_weather_agent at 0x7998740a7d00>], tool_choice=None, parallel_tool_calls=True) context_variables={}\n",
            "=============\n",
            "=============\n",
            "The answer to 2 + 2 is 4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"How is weather in Canada in December?\"}]\n",
        "response = client.run(agent=math_agent, messages=messages)\n",
        "print(response)\n",
        "print(\"=============\")\n",
        "print(\"=============\")\n",
        "print(response.messages[-1][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdLnwkIWEYjK",
        "outputId": "2e010da1-a845-4afd-e0bc-6a5d434d5e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handing off to Weather Agent\n",
            "messages=[{'content': None, 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': [{'id': 'call_CaQyJ9v8wh7YRVvRQrCvkh33', 'function': {'arguments': '{}', 'name': 'handoff_to_weather_agent'}, 'type': 'function'}], 'sender': 'Math Agent'}, {'role': 'tool', 'tool_call_id': 'call_CaQyJ9v8wh7YRVvRQrCvkh33', 'tool_name': 'handoff_to_weather_agent', 'content': '{\"assistant\": \"Weather Agent\"}'}, {'content': \"I'm not currently set up to provide a direct response to your weather query, but I can refer you to a weather service for specific and up-to-date weather forecasts in Canada for December. Canada generally experiences cold weather during December, with varying conditions depending on the region. Coastal areas like Vancouver may have milder temperatures, while areas like Toronto, Ottawa, and Montreal see cooler, snowy conditions. Northern regions experience extreme cold. Checking a reliable weather service will provide the most accurate information.\", 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None, 'sender': 'Weather Agent'}] agent=Agent(name='Weather Agent', model='gpt-4o', instructions='You handle only weather-related queries.', functions=[<function handoff_to_math_agent at 0x79984fd27400>], tool_choice=None, parallel_tool_calls=True) context_variables={}\n",
            "=============\n",
            "=============\n",
            "I'm not currently set up to provide a direct response to your weather query, but I can refer you to a weather service for specific and up-to-date weather forecasts in Canada for December. Canada generally experiences cold weather during December, with varying conditions depending on the region. Coastal areas like Vancouver may have milder temperatures, while areas like Toronto, Ottawa, and Montreal see cooler, snowy conditions. Northern regions experience extreme cold. Checking a reliable weather service will provide the most accurate information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating RAG and NL2SQL Agent"
      ],
      "metadata": {
        "id": "6J83WHEx4qCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-chroma langchain-openai langchain-community pypdf sentence-transformers"
      ],
      "metadata": {
        "id": "m63flywZ8SmP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "import os\n",
        "\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if filename.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith('.docx'):\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {filename}\")\n",
        "            continue\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "folder_path = \"/content/docs\"\n",
        "documents = load_documents(folder_path)\n",
        "print(f\"Loaded {len(documents)} documents from the folder.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXzj73-xiCFd",
        "outputId": "b2303ba3-815c-44be-bd23-c26e497ed171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2 documents from the folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EFJzOcLjQww",
        "outputId": "81db640b-1a48-4348-e259-474aa0659f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split the documents into 4 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "collection_name = \"my_collection\"\n",
        "vectorstore = Chroma.from_documents(\n",
        "    collection_name=collection_name,\n",
        "    documents=splits,\n",
        "    embedding=embedding_function,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "print(\"Vector store created and persisted to './chroma_db'\")\n"
      ],
      "metadata": {
        "id": "lWd5RocqjiMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "retriever_results = retriever.invoke(\"Who was the founder of Futuresmart AI?\")\n",
        "print(retriever_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCujjMoJj21F",
        "outputId": "ab91d1a5-22ce-44df-f820-f7224f9dc3ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'page': 1, 'source': '/content/docs/FutureSmart AI .pdf'}, page_content='FutureSmart AI provides customized speech to text services, employing cutting -\\nedge speech recognition technologies to cater to specific client needs. Ideal for \\ncreating efficient documen tation and enabling voice -driven commands, this \\nsolution boosts productivity and accessibility.'), Document(metadata={'page': 0, 'source': '/content/docs/FutureSmart AI .pdf'}, page_content='FutureSmart AI provides custom Natural Language Processing (NLP) \\nsolutions for companies looking to get ahead of the future. Our \\ndedicated team of Data Scientists and ML Engineers provides an end -\\nto-end solution from data labeling to modeling and deploying an ML \\nmodel tailored to your specific use case . \\nFounder: Pradip Nichite  \\n \\nServices:  \\nText Classification  \\nAt FutureSmart AI, we develop custom text classificati on solutions using \\nadvanced NLP techniques tailored to your specific business requirements. \\nLeveraging Python, Pytorch, and Hugging Face transformers, we enable precise \\ndata categorization across applications such as intent detection, document \\ncategorizati on, and sentiment analysis, enhancing your decision -making \\nprocesses and operational efficiency.  \\n \\nChatbots  \\nWe specialize in creating custom chatbots that integrate seamlessly with your \\nbusiness environment. Using semantic search and large language models,  our')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "\n",
        "def retrieve_and_generate(question):\n",
        "  template = \"\"\"Answer the question based only on the following context:\n",
        "  {context}\n",
        "  Question: {question}\n",
        "  Answer: \"\"\"\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "  def docs2str(docs):\n",
        "      return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "  rag_chain = (\n",
        "      {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
        "      | prompt\n",
        "      | llm\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "  response = rag_chain.invoke(question)\n",
        "  return response\n",
        "\n",
        "question = \"Who was the founder of Futuresmart AI?\"\n",
        "result = retrieve_and_generate(question)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqMJu2PokicD",
        "outputId": "2acd93ef-1320-41ce-80fe-adb463d9415c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who was the founder of Futuresmart AI?\n",
            "Answer: The founder of FutureSmart AI is Pradip Nichite.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up DB for NL2SQL"
      ],
      "metadata": {
        "id": "TUeLx41DcCyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite"
      ],
      "metadata": {
        "id": "kP5AjefF3cfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv Chinook_Sqlite.sqlite Chinook.db"
      ],
      "metadata": {
        "id": "m3s5oO6b3guc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import SQLDatabase\n",
        "\n",
        "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")"
      ],
      "metadata": {
        "id": "3i3OP-lm3jdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_sql_query_chain\n",
        "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
        "from operator import itemgetter\n",
        "import re\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "def sql_response_gen(question):\n",
        "  remove_code_block_syntax = lambda text: re.sub(r\"```(sql|)\\s*(.*?)\\s*```\", r\"\\2\", text, flags=re.DOTALL)\n",
        "  execute_query = QuerySQLDataBaseTool(db=db)\n",
        "  write_query = create_sql_query_chain(llm, db)\n",
        "\n",
        "  answer_prompt = PromptTemplate.from_template(\n",
        "      \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
        "\n",
        "  Question: {question}\n",
        "  SQL Query: {query}\n",
        "  SQL Result: {result}\n",
        "  Answer: \"\"\"\n",
        "  )\n",
        "\n",
        "  chain = (\n",
        "      RunnablePassthrough.assign(query=write_query | RunnableLambda(remove_code_block_syntax)).assign(\n",
        "          result=itemgetter(\"query\") | execute_query\n",
        "      )\n",
        "      | answer_prompt\n",
        "      | llm\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "\n",
        "  response = chain.invoke({\"question\": question})\n",
        "  return response\n",
        "\n",
        "question = \"How many employees are there?\"\n",
        "result = sql_response_gen(question)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7VUiXx93VDh",
        "outputId": "9c527c45-02f9-4c0e-c8ec-8977cc284a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How many employees are there?\n",
            "Answer: There are 8 employees.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from swarm import Swarm, Agent\n",
        "\n",
        "rag_agent = Agent(\n",
        "    name=\"RAG Agent\",\n",
        "    instructions=\"You retrieve relevant information from the company's knowledge base and generate responses to general queries about the company.\",\n",
        "    functions=[retrieve_and_generate]\n",
        ")\n",
        "\n",
        "nl2sql_agent = Agent(\n",
        "    name=\"NL2SQL Agent\",\n",
        "    instructions=\"You handle database queries.\",\n",
        "    functions=[sql_response_gen]\n",
        ")"
      ],
      "metadata": {
        "id": "uXrbLlBRlD8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "central_agent = Agent(\n",
        "    name=\"Central Agent\",\n",
        "    instructions=\"Determine if the query is about general company information (RAG) or a database query (NL2SQL), and route the query accordingly.\"\n",
        ")\n",
        "\n",
        "# Define handoff functions to delegate tasks to the correct agent\n",
        "def transfer_to_nl2sql():\n",
        "    \"\"\"Transfer the task to the NL2SQL Agent for database queries.\"\"\"\n",
        "    return nl2sql_agent\n",
        "\n",
        "def transfer_to_rag():\n",
        "    \"\"\"Transfer the task to the RAG Agent for general company queries.\"\"\"\n",
        "    return rag_agent\n",
        "\n",
        "# Attach the handoff functions to the central agent\n",
        "central_agent.functions = [transfer_to_nl2sql, transfer_to_rag]"
      ],
      "metadata": {
        "id": "SobSdFVtnVeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Swarm()\n",
        "\n",
        "# Example 1: Asking about the company\n",
        "print(\"\\n--- Example 1: Asking about the company ---\")\n",
        "messages = [{\"role\": \"user\", \"content\": \"What does Futuresmart AI offer?\"}]\n",
        "response = client.run(agent=central_agent, messages=messages)\n",
        "if isinstance(response, Agent):\n",
        "    selected_agent = response\n",
        "    result = selected_agent.functions\n",
        "    print(result)\n",
        "else:\n",
        "    print(response.messages[-1][\"content\"])\n",
        "\n",
        "\n",
        "# Example 2: Asking from the SQL DB\n",
        "print(\"\\n--- Example 2: Asking from the SQL DB ---\")\n",
        "messages = [{\"role\": \"user\", \"content\": \"How many employees are there in the database?\"}]\n",
        "response = client.run(agent=central_agent, messages=messages)\n",
        "if isinstance(response, Agent):\n",
        "    selected_agent = response\n",
        "    result = selected_agent.functions\n",
        "    print(result)\n",
        "else:\n",
        "    print(response.messages[-1][\"content\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud7hhK_7lxhT",
        "outputId": "d6c10dde-c6c6-401f-b0a3-413e4af08dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 1: Asking about the company ---\n",
            "FutureSmart AI offers a range of services including customized speech-to-text services, Natural Language Processing (NLP) solutions, text classification, and the creation of custom chatbots. These services are designed to enhance productivity, accessibility, decision-making processes, and operational efficiency through advanced technologies and tailored solutions.\n",
            "\n",
            "--- Example 2: Asking from the SQL DB ---\n",
            "There are 8 employees in the database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "creFnFdMr4_m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}